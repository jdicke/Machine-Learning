{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-french.pkl\n",
      "[go] => [va]\n",
      "[hi] => [salut]\n",
      "[run] => [cours]\n",
      "[run] => [courez]\n",
      "[wow] => [ca alors]\n",
      "[fire] => [au feu]\n",
      "[help] => [a laide]\n",
      "[jump] => [saute]\n",
      "[stop] => [ca suffit]\n",
      "[stop] => [stop]\n",
      "[stop] => [arretetoi]\n",
      "[wait] => [attends]\n",
      "[wait] => [attendez]\n",
      "[go on] => [poursuis]\n",
      "[go on] => [continuez]\n",
      "[go on] => [poursuivez]\n",
      "[hello] => [bonjour]\n",
      "[hello] => [salut]\n",
      "[i see] => [je comprends]\n",
      "[i try] => [jessaye]\n",
      "[i won] => [jai gagne]\n",
      "[i won] => [je lai emporte]\n",
      "[oh no] => [oh non]\n",
      "[attack] => [attaque]\n",
      "[attack] => [attaquez]\n",
      "[cheers] => [sante]\n",
      "[cheers] => [a votre sante]\n",
      "[cheers] => [merci]\n",
      "[cheers] => [tchintchin]\n",
      "[get up] => [levetoi]\n",
      "[go now] => [va maintenant]\n",
      "[go now] => [allezy maintenant]\n",
      "[go now] => [vasy maintenant]\n",
      "[got it] => [jai pige]\n",
      "[got it] => [compris]\n",
      "[got it] => [pige]\n",
      "[got it] => [compris]\n",
      "[got it] => [tas capte]\n",
      "[hop in] => [monte]\n",
      "[hop in] => [montez]\n",
      "[hug me] => [serremoi dans tes bras]\n",
      "[hug me] => [serrezmoi dans vos bras]\n",
      "[i fell] => [je suis tombee]\n",
      "[i fell] => [je suis tombe]\n",
      "[i know] => [je sais]\n",
      "[i left] => [je suis parti]\n",
      "[i left] => [je suis partie]\n",
      "[i lost] => [jai perdu]\n",
      "[im] => [jai ans]\n",
      "[im ok] => [je vais bien]\n",
      "[im ok] => [ca va]\n",
      "[listen] => [ecoutez]\n",
      "[no way] => [cest pas possible]\n",
      "[no way] => [impossible]\n",
      "[no way] => [en aucun cas]\n",
      "[no way] => [sans facons]\n",
      "[no way] => [cest hors de question]\n",
      "[no way] => [il nen est pas question]\n",
      "[no way] => [cest exclu]\n",
      "[no way] => [en aucune maniere]\n",
      "[no way] => [hors de question]\n",
      "[really] => [vraiment]\n",
      "[really] => [vrai]\n",
      "[really] => [ah bon]\n",
      "[thanks] => [merci]\n",
      "[we try] => [on essaye]\n",
      "[we won] => [nous avons gagne]\n",
      "[we won] => [nous gagnames]\n",
      "[we won] => [nous lavons emporte]\n",
      "[we won] => [nous lemportames]\n",
      "[ask tom] => [demande a tom]\n",
      "[awesome] => [fantastique]\n",
      "[be calm] => [sois calme]\n",
      "[be calm] => [soyez calme]\n",
      "[be calm] => [soyez calmes]\n",
      "[be cool] => [sois detendu]\n",
      "[be fair] => [sois juste]\n",
      "[be fair] => [soyez juste]\n",
      "[be fair] => [soyez justes]\n",
      "[be fair] => [sois equitable]\n",
      "[be fair] => [soyez equitable]\n",
      "[be fair] => [soyez equitables]\n",
      "[be kind] => [sois gentil]\n",
      "[be nice] => [sois gentil]\n",
      "[be nice] => [sois gentille]\n",
      "[be nice] => [soyez gentil]\n",
      "[be nice] => [soyez gentille]\n",
      "[be nice] => [soyez gentils]\n",
      "[be nice] => [soyez gentilles]\n",
      "[beat it] => [degage]\n",
      "[call me] => [appellemoi]\n",
      "[call me] => [appellezmoi]\n",
      "[call us] => [appellenous]\n",
      "[call us] => [appeleznous]\n",
      "[come in] => [entrez]\n",
      "[come in] => [entre]\n",
      "[come in] => [entre]\n",
      "[come in] => [entrez]\n",
      "[come on] => [allez]\n",
      "[come on] => [allez]\n",
      "Saved: english-french-both.pkl\n",
      "Saved: english-french-train.pkl\n",
      "Saved: english-french-test.pkl\n",
      "English Vocabulary Size: 2125\n",
      "English Max Length: 5\n",
      "French Vocabulary Size: 4397\n",
      "French Max Length: 10\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 10, 256)           1125632   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 5, 2125)           546125    \n",
      "=================================================================\n",
      "Total params: 2,722,381\n",
      "Trainable params: 2,722,381\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      " - 48s - loss: 4.2405 - val_loss: 3.4816\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.48164, saving model to model.h5\n",
      "Epoch 2/30\n",
      " - 44s - loss: 3.3005 - val_loss: 3.2949\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.48164 to 3.29488, saving model to model.h5\n",
      "Epoch 3/30\n",
      " - 42s - loss: 3.1334 - val_loss: 3.2045\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.29488 to 3.20446, saving model to model.h5\n",
      "Epoch 4/30\n",
      " - 43s - loss: 2.9915 - val_loss: 3.0784\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.20446 to 3.07841, saving model to model.h5\n",
      "Epoch 5/30\n",
      " - 42s - loss: 2.7994 - val_loss: 2.9417\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.07841 to 2.94173, saving model to model.h5\n",
      "Epoch 6/30\n",
      " - 43s - loss: 2.6256 - val_loss: 2.8418\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.94173 to 2.84176, saving model to model.h5\n",
      "Epoch 7/30\n",
      " - 41s - loss: 2.4634 - val_loss: 2.7505\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.84176 to 2.75055, saving model to model.h5\n",
      "Epoch 8/30\n",
      " - 42s - loss: 2.3068 - val_loss: 2.6312\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.75055 to 2.63124, saving model to model.h5\n",
      "Epoch 9/30\n",
      " - 42s - loss: 2.1457 - val_loss: 2.5460\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.63124 to 2.54604, saving model to model.h5\n",
      "Epoch 10/30\n",
      " - 42s - loss: 1.9960 - val_loss: 2.4423\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.54604 to 2.44232, saving model to model.h5\n",
      "Epoch 11/30\n",
      " - 41s - loss: 1.8490 - val_loss: 2.3752\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.44232 to 2.37520, saving model to model.h5\n",
      "Epoch 12/30\n",
      " - 42s - loss: 1.7222 - val_loss: 2.3026\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.37520 to 2.30262, saving model to model.h5\n",
      "Epoch 13/30\n",
      " - 41s - loss: 1.6028 - val_loss: 2.2469\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.30262 to 2.24692, saving model to model.h5\n",
      "Epoch 14/30\n",
      " - 44s - loss: 1.4909 - val_loss: 2.2052\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.24692 to 2.20519, saving model to model.h5\n",
      "Epoch 15/30\n",
      " - 41s - loss: 1.3855 - val_loss: 2.1690\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.20519 to 2.16900, saving model to model.h5\n",
      "Epoch 16/30\n",
      " - 42s - loss: 1.2822 - val_loss: 2.1203\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.16900 to 2.12033, saving model to model.h5\n",
      "Epoch 17/30\n",
      " - 42s - loss: 1.1844 - val_loss: 2.0874\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.12033 to 2.08736, saving model to model.h5\n",
      "Epoch 18/30\n",
      " - 43s - loss: 1.0903 - val_loss: 2.0485\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.08736 to 2.04854, saving model to model.h5\n",
      "Epoch 19/30\n",
      " - 44s - loss: 1.0034 - val_loss: 2.0231\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.04854 to 2.02305, saving model to model.h5\n",
      "Epoch 20/30\n",
      " - 44s - loss: 0.9192 - val_loss: 2.0123\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.02305 to 2.01225, saving model to model.h5\n",
      "Epoch 21/30\n",
      " - 43s - loss: 0.8432 - val_loss: 1.9912\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.01225 to 1.99118, saving model to model.h5\n",
      "Epoch 22/30\n",
      " - 44s - loss: 0.7695 - val_loss: 1.9690\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.99118 to 1.96901, saving model to model.h5\n",
      "Epoch 23/30\n",
      " - 44s - loss: 0.7013 - val_loss: 1.9435\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.96901 to 1.94349, saving model to model.h5\n",
      "Epoch 24/30\n",
      " - 44s - loss: 0.6391 - val_loss: 1.9441\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.94349\n",
      "Epoch 25/30\n",
      " - 44s - loss: 0.5810 - val_loss: 1.9376\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.94349 to 1.93765, saving model to model.h5\n",
      "Epoch 26/30\n",
      " - 43s - loss: 0.5288 - val_loss: 1.9168\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.93765 to 1.91679, saving model to model.h5\n",
      "Epoch 27/30\n",
      " - 43s - loss: 0.4819 - val_loss: 1.9240\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.91679\n",
      "Epoch 28/30\n",
      " - 45s - loss: 0.4385 - val_loss: 1.9185\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.91679\n",
      "Epoch 29/30\n",
      " - 44s - loss: 0.4007 - val_loss: 1.9207\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.91679\n",
      "Epoch 30/30\n",
      " - 45s - loss: 0.3656 - val_loss: 1.9116\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.91679 to 1.91162, saving model to model.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x250ac5b1390>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Load document\n",
    "def load_doc(filename):\n",
    "    #Open as read\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# Split a loadead document into sentences\n",
    "# Splits by line and then by phrase\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "    return pairs\n",
    "\n",
    "# Clean data\n",
    "# Removes all non-printable chars\n",
    "# Remove punctuation\n",
    "# Normalize to ASCII chars\n",
    "# Put to lowercase\n",
    "# Remove anything non-alphabetic\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # Char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # For removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        \n",
    "        for line in pair:\n",
    "            # Normalize unicode\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            line = line.split()\n",
    "            \n",
    "            # Convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            \n",
    "            # Remove punctuation\n",
    "            line = [word.translate(table) for word in line]\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            \n",
    "            # Store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)\n",
    "\n",
    "# Save list to file\n",
    "def save_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "    \n",
    "# Load in dataset\n",
    "filename = 'fra.txt'\n",
    "doc = load_doc(filename)\n",
    "pairs = to_pairs(doc)\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "\n",
    "save_data(clean_pairs, 'english-french.pkl')\n",
    "\n",
    "# Spot check\n",
    "for i in range(100):\n",
    "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))\n",
    "    \n",
    "    \n",
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "    \n",
    "# Load dataset\n",
    "raw_dataset = load_clean_sentences('english-french.pkl')\n",
    "\n",
    "# Reduce the dataset size for simplicity\n",
    "# Because there are 150,000 examples and the training would take much longer to go through all of that\n",
    "n_sentences = 10000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "\n",
    "# Random shuffle\n",
    "shuffle(dataset)\n",
    "\n",
    "# Split into train set and test set\n",
    "# 9000 for the training, 1000 for the testing\n",
    "train, test = dataset[:9000], dataset[9000:]\n",
    "\n",
    "# Save\n",
    "save_clean_data(dataset, 'english-french-both.pkl')\n",
    "save_clean_data(train, 'english-french-train.pkl')\n",
    "save_clean_data(test, 'english-french-test.pkl')\n",
    "\n",
    "def load_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# Load data\n",
    "dataset = load_sentences('english-french-both.pkl')\n",
    "train = load_sentences('english-french-train.pkl')\n",
    "test = load_sentences('english-french-test.pkl')\n",
    "\n",
    "# Tokenizer - converts a sequence of characters to a sequence of tokens\n",
    "# Maps words to integers\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    "\n",
    "# Prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "\n",
    "# Prepare french tokenizer\n",
    "fra_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "fra_length = max_length(dataset[:, 1])\n",
    "print('French Vocabulary Size: %d' % fra_vocab_size)\n",
    "print('French Max Length: %d' % (fra_length))\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    "\n",
    "# Prepare training data\n",
    "trainX = encode_sequences(fra_tokenizer, fra_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "\n",
    "# Prepare validation data\n",
    "testX = encode_sequences(fra_tokenizer, fra_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    "\n",
    "# Define Model\n",
    "# Uses encoder-decoder LSTM model - the input sequence is encoded by a front-end model 'the encoder'\n",
    "# then decoded word by word by a backend model 'the decoder'\n",
    "\n",
    "# define_model() defines the model and takes the size of the input and output vocabularies, \n",
    "# the max length of input and output phrases, \n",
    "# and the number of memory units used to configure the model\n",
    "\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model\n",
    " \n",
    "# define model - adam approach is stochastic gradient descent - multi-class classification\n",
    "model = define_model(fra_vocab_size, eng_vocab_size, fra_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "# plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "# Fit model and train the model - 30 epochs and 64 batch examples\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
